{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cdm encoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.unet import *\n",
    "from models.nn import (\n",
    "    SiLU,\n",
    "    conv_nd,\n",
    "    linear,\n",
    "    avg_pool_nd,\n",
    "    zero_module,\n",
    "    normalization,\n",
    "    timestep_embedding,\n",
    "    checkpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_channels, \n",
    "                out_channels,\n",
    "                emb_channels=None,\n",
    "                dropout=0.1,\n",
    "                conv_resample=True,\n",
    "                use_scale_shift_norm=False,\n",
    "                dims=2,\n",
    "                use_checkpoint=False,):\n",
    "        super().__init__()\n",
    "        self.resblock = ResBlock(\n",
    "                in_channels,\n",
    "                emb_channels,\n",
    "                dropout,\n",
    "                out_channels=out_channels,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            )\n",
    "        self.conv = TimestepEmbedSequential(Downsample(out_channels, conv_resample, dims=dims))\n",
    "\n",
    "    def forward(self, x, emb=None):\n",
    "        x = self.resblock(x, emb)\n",
    "        x = self.conv(x, emb)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_classes': 10, 'dataset': 'cifar10', 'data_dir': '../data/cifar_train', 'log_dir': './log/cdm_log', 'model_path': './log/jscc/model100000.pt', 'encoder_path': './log/unet/encoder100000.pt', 'jscc_encoder_path': './log/jscc/encoder100000.pt', 'dropout': 0.3, 'lr': 0.0001, 'weight_decay': 0.0, 'lr_anneal_steps': 100000, 'batch_size': 64, 'microbatch': -1, 'ema_rate': 0.9999, 'log_interval': 100, 'save_interval': 50000, 'resume_checkpoint': '', 'use_fp16': False, 'fp16_scale_growth': '1e-3', 'image_size': 32, 'num_channels': 128, 'num_res_blocks': 3, 'num_heads': 4, 'num_heads_upsample': -1, 'attention_resolutions': '16,8', 'learn_sigma': True, 'diffusion_steps': 4000, 'noise_schedule': 'cosine', 'schedule_sampler': 'uniform', 'sigma_small': False, 'class_cond': True, 'timestep_respacing': '', 'use_kl': False, 'predict_xstart': False, 'rescale_timesteps': True, 'rescale_learned_sigmas': True, 'use_checkpoint': False, 'use_scale_shift_norm': True, 'num_samples': 100, 'clip_denoised': True, 'use_ddim': True, 'encoder_type': 'cdm', 'use_label_embed': False, 'use_time_embed': False, 'use_latent': True, 'latent_dim': 512, 'out_channels': 512, 'hidden_dims': [12, 24, 48, 96, 512], 'c_out': 8, 'noise': 'Gaussian'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open('configs/cifar10.yaml', 'r') as f:\n",
    "    try:\n",
    "        config = yaml.safe_load(f)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['num_classes', 'dataset', 'data_dir', 'log_dir', 'model_path', 'encoder_path', 'jscc_encoder_path', 'dropout', 'lr', 'weight_decay', 'lr_anneal_steps', 'batch_size', 'microbatch', 'ema_rate', 'log_interval', 'save_interval', 'resume_checkpoint', 'use_fp16', 'fp16_scale_growth', 'image_size', 'num_channels', 'num_res_blocks', 'num_heads', 'num_heads_upsample', 'attention_resolutions', 'learn_sigma', 'diffusion_steps', 'noise_schedule', 'schedule_sampler', 'sigma_small', 'class_cond', 'timestep_respacing', 'use_kl', 'predict_xstart', 'rescale_timesteps', 'rescale_learned_sigmas', 'use_checkpoint', 'use_scale_shift_norm', 'num_samples', 'clip_denoised', 'use_ddim', 'encoder_type', 'use_label_embed', 'use_time_embed', 'use_latent', 'latent_dim', 'out_channels', 'hidden_dims', 'c_out', 'noise'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "from models.image_datasets import load_data\n",
    "data = load_data(\n",
    "    data_dir=config['data_dir'],\n",
    "    batch_size=config['batch_size'],\n",
    "    image_size=config['image_size'],\n",
    "    class_cond=config['class_cond'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        model_channels,\n",
    "        out_channels,\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 4, 8),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        num_classes=None,\n",
    "        use_checkpoint=False,\n",
    "        use_scale_shift_norm=False,\n",
    "        use_time_embed=False,\n",
    "        use_label_embed=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_classes = num_classes\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.use_time_embed = use_time_embed\n",
    "        self.use_label_embed = use_label_embed\n",
    "\n",
    "        if use_time_embed:\n",
    "            time_embed_dim = model_channels * 4\n",
    "            self.time_embed = nn.Sequential(\n",
    "                linear(model_channels, time_embed_dim),\n",
    "                SiLU(),\n",
    "                linear(time_embed_dim, time_embed_dim),\n",
    "            )\n",
    "        else:\n",
    "            time_embed_dim = None\n",
    "\n",
    "        assert (use_label_embed) == (\n",
    "            self.num_classes is not None\n",
    "        ), \"must specify num_classes if and only if the model is class-conditional\"\n",
    "\n",
    "        if self.use_label_embed:\n",
    "            label_embed_dim = model_channels * 4\n",
    "            self.label_emb = nn.Embedding(num_classes, label_embed_dim)\n",
    "        else:\n",
    "            label_embed_dim = None\n",
    "\n",
    "        embed_dim = time_embed_dim or label_embed_dim\n",
    "        ch = model_channels\n",
    "        self.input_blocks = nn.ModuleList(\n",
    "            [\n",
    "                TimestepEmbedSequential(\n",
    "                    conv_nd(dims, in_channels, model_channels, 3, padding=1)\n",
    "                )\n",
    "            ]\n",
    "            )\n",
    "\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            layers = [\n",
    "                EncBlock(in_channels=ch,\n",
    "                    out_channels=mult * model_channels,\n",
    "                    emb_channels=embed_dim,\n",
    "                    dropout=dropout,\n",
    "                    conv_resample=True,\n",
    "                    use_scale_shift_norm=False,\n",
    "                    dims=2,\n",
    "                    use_checkpoint=False,)\n",
    "            ]\n",
    "            ch = mult * model_channels\n",
    "            self.input_blocks.append(\n",
    "                TimestepEmbedSequential(*layers)\n",
    "            )\n",
    "        self.out = nn.Sequential(\n",
    "            normalization(ch),\n",
    "            nn.SiLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            conv_nd(dims, ch, out_channels, 1),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        # self.input_blocks.append(TimestepEmbedSequential(\n",
    "        #             conv_nd(dims, ch, out_channels, 3, padding=1)\n",
    "        #         ))\n",
    "    def forward(self, x, timesteps=None, y=None):\n",
    "        if self.use_time_embed:\n",
    "            emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
    "        else:\n",
    "            emb = None\n",
    "        # label embedding\n",
    "        if self.use_label_embed:\n",
    "            assert y.shape == (x.shape[0],), \"labels are not provided or are not the right shape\"\n",
    "            # y: [64]; label_emb: [64, 512]\n",
    "            if emb is None:\n",
    "                emb = self.label_emb(y)\n",
    "            else:\n",
    "                emb = emb + self.label_emb(y)\n",
    "        for block in self.input_blocks:\n",
    "            x = block(x, emb)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = encoder(in_channels=3,\n",
    "        model_channels=128,\n",
    "        out_channels=512,\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 4, 8),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        num_classes=None,\n",
    "        use_checkpoint=False,\n",
    "        use_scale_shift_norm=False,\n",
    "        use_time_embed=False,\n",
    "        use_label_embed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 512])\n"
     ]
    }
   ],
   "source": [
    "batch, cond = next(data)\n",
    "temp = Encoder(batch)\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder(\n",
       "  (input_blocks): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): TimestepEmbedSequential(\n",
       "      (0): EncBlock(\n",
       "        (resblock): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (emb_layers): Identity()\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (conv): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TimestepEmbedSequential(\n",
       "      (0): EncBlock(\n",
       "        (resblock): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (emb_layers): Identity()\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): EncBlock(\n",
       "        (resblock): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (emb_layers): Identity()\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): EncBlock(\n",
       "        (resblock): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (emb_layers): Identity()\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "    (1): SiLU()\n",
       "    (2): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ec660b0ea2d828e76e2eed44f0430f21c361a11018087dd77967b17f7ee22b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
